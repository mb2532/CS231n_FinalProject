{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17218,"status":"ok","timestamp":1652025383423,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"QwCYzowylIjb","outputId":"eadf71e1-d47e-43db-996f-09ef121915c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4198,"status":"ok","timestamp":1652025387618,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"-EQThYJflKmi","outputId":"dc10472e-0ef5-4fe2-9407-cf0f4a613552"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch version：1.11.0+cu113\n","torchvision version: 0.12.0+cu113\n","CUDA Version: 11.3\n","cuDNN version is: 8200\n"]}],"source":["import torch,torchvision\n","print(f\"torch version：{torch.__version__}\")\n","print(f\"torchvision version: {torchvision.__version__}\")\n","print(f\"CUDA Version: {torch.version.cuda}\")\n","print(f\"cuDNN version is: {torch.backends.cudnn.version()}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"67FHvRnRj8pm","executionInfo":{"status":"ok","timestamp":1652025387618,"user_tz":420,"elapsed":16,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torchvision.transforms as T"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18294,"status":"ok","timestamp":1652025405898,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"HCM9Q6gCSZut","outputId":"cdd61da3-a5c6-4ce6-95ec-b6502a4302b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torchvision 0.12.0+cu113\n","Uninstalling torchvision-0.12.0+cu113:\n","  Would remove:\n","    /usr/local/lib/python3.7/dist-packages/torchvision-0.12.0+cu113.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libcudart.053364c0.so.11.0\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libnvjpeg.90286a3c.so.11\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libz.1328edc3.so.1\n","    /usr/local/lib/python3.7/dist-packages/torchvision/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torchvision-0.12.0+cu113\n"]}],"source":["!pip uninstall torchvision"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":92111,"status":"ok","timestamp":1652025498002,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"PCTqB2mYjnGf","outputId":"08e73d2d-ec6d-4b60-9617-b16f804e7f3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorchvideo\n","  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 5.1 MB/s \n","\u001b[?25hCollecting av\n","  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\n","\u001b[K     |████████████████████████████████| 28.2 MB 3.0 MB/s \n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20220506.tar.gz (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 3.4 MB/s \n","\u001b[?25hCollecting parameterized\n","  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n","Collecting iopath\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (2.6.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.21.6)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.64.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Building wheels for collected packages: pytorchvideo, fvcore\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=20be78d0c8366d3b4fda0d9136bcb59a42db3ad3d678bd671c423f52052830be\n","  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20220506-py3-none-any.whl size=61284 sha256=33f5239295a7d1f10829da01d612da7af81012668270931890f4d9f46293f833\n","  Stored in directory: /root/.cache/pip/wheels/f5/ef/3c/708de8799f89f0871bd209866831fe3885db93fa090608fa73\n","Successfully built pytorchvideo fvcore\n","Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n","Successfully installed av-9.2.0 fvcore-0.1.5.post20220506 iopath-0.1.9 parameterized-0.8.1 portalocker-2.4.0 pytorchvideo-0.1.5 pyyaml-6.0 yacs-0.1.8\n","Requirement already satisfied: av in /usr/local/lib/python3.7/dist-packages (9.2.0)\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Collecting torchvision\n","  Downloading https://download.pytorch.org/whl/nightly/cpu/torchvision-0.13.0.dev20220508%2Bcpu-cp37-cp37m-linux_x86_64.whl (13.5 MB)\n","\u001b[K     |████████████████████████████████| 13.5 MB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.12.0.dev20220508%2Bcpu-cp37-cp37m-linux_x86_64.whl (186.7 MB)\n","\u001b[K     |████████████████████████████████| 186.7 MB 22 kB/s \n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Collecting torchaudio\n","  Downloading https://download.pytorch.org/whl/nightly/cpu/torchaudio-0.12.0.dev20220508%2Bcpu-cp37-cp37m-linux_x86_64.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.11.0+cu113\n","    Uninstalling torchaudio-0.11.0+cu113:\n","      Successfully uninstalled torchaudio-0.11.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.12.0.dev20220508+cpu which is incompatible.\u001b[0m\n","Successfully installed torch-1.12.0.dev20220508+cpu torchaudio-0.12.0.dev20220508+cpu torchvision-0.13.0.dev20220508+cpu\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchvision"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["9.2.0\n"]}],"source":["!pip install pytorchvideo av\n","!pip install av\n","!pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n","import pytorchvideo\n","import av\n","print(av.__version__)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_zigZRXVkBVX","executionInfo":{"status":"ok","timestamp":1652025498003,"user_tz":420,"elapsed":13,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1652025498565,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"NHBH7qy9uzQS","outputId":"b9449d5a-3541-4294-ea51-1b753eb39083"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1reDWkXcIczcWArSj0UXBBRo24d0byImu/CS_231n_Project/ConvLSTM\n"]}],"source":["FOLDERNAME = 'CS_231n_Project/ConvLSTM'\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4bu5-fKNlMg6","executionInfo":{"status":"ok","timestamp":1652025498566,"user_tz":420,"elapsed":5,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["from torchvision.datasets import UCF101\n","\n","# These are some minimal variables used to configure and load the dataset:\n","ucf_data_dir = \"../UCF101_sample/UCF-101\"\n","ucf_label_dir = \"../UCF101_sample/TrainTestList\"\n","frames_per_clip =5\n","step_between_clips = 5\n","batch_size = 64"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ZzkC66YOt4tf","executionInfo":{"status":"ok","timestamp":1652025500001,"user_tz":420,"elapsed":1440,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from Seq2Seq import Seq2Seq\n","from torch.utils.data import DataLoader\n","\n","import io\n","import imageio\n","from ipywidgets import widgets, HBox\n","\n","# Use GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"56A6U7vIO_lF","executionInfo":{"status":"ok","timestamp":1652025500002,"user_tz":420,"elapsed":5,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["tfs = T.Compose([\n","    # TODO: this should be done by a video-level transfrom when PyTorch provides transforms.ToTensor() for video\n","    # scale in [0, 1] of type float\n","    # T.Lambda(lambda x: x / 255.),\n","    # reshape into (C,T,H,W) from (T, H, W, C) for easier convolutions #### (to match ConvLSTM stuff)\n","    # might need C in index 1\n","    T.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n","    # rescale to the most common size\n","    T.Lambda(lambda x: nn.functional.interpolate(x, (64, 64))),\n","])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"rn31TitrkRGN","executionInfo":{"status":"ok","timestamp":1652025500002,"user_tz":420,"elapsed":4,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["# def custom_collate(batch):\n","#     filtered_batch = []\n","#     for video, _, label in batch:\n","#         filtered_batch.append((video, label))\n","#     return torch.utils.data.dataloader.default_collate(filtered_batch)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TAGfg2Tfv6RA","executionInfo":{"status":"ok","timestamp":1652025500003,"user_tz":420,"elapsed":5,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["def custom_collate(batch):\n","\n","    # Add channel dim, scale pixels between 0 and 1, send to GPU\n","    filtered_batch = []\n","    for video, _, _ in batch:\n","      filtered_batch.append(video)\n","    \n","    # new_batch = torch.utils.data.dataloader.default_collate(filtered_batch)\n","    \n","    new_batch = torch.stack(filtered_batch)   \n","    new_batch = new_batch / 255.0                        \n","    new_batch = new_batch.to(device)                   \n","\n","    # first 4 frames are input, 5th frame is target               \n","    return new_batch[:,:,0:4, :, :], new_batch[:,:,4, :, :]     \n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885,"referenced_widgets":["80184dae3d69414d9fcd3adf809a7e67","f98046bf0baf4fe3a1456177d191e098","1040e8183e04482b8d400cffca68e609","2d775ac035254e42ac8b3373372d31db","2d3d7a862cff4e5bb2a7e39040d946a9","df11f03a91854b8f811d3875327b8714","e04bd0d3cd45401fb8727373f1aa004f","23324c2d6aa64b00821fdcc58d75ce53","5bd355d158ca42989a55e17f671862d0","79470611b15041fcb197cea6b8cb94f6","f21cb511a5204971966049436380920d"]},"executionInfo":{"elapsed":4197,"status":"error","timestamp":1652025504196,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"oY8nKu8NPETm","outputId":"734eee67-1c05-469a-8b0c-af8deb50675a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80184dae3d69414d9fcd3adf809a7e67"}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d3f4657ddd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n\u001b[0;32m----> 4\u001b[0;31m                        step_between_clips=step_between_clips, train=True, transform=tfs)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                                           collate_fn=custom_collate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/ucf101.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annotation_path, frames_per_clip, step_between_clips, frame_rate, fold, train, transform, _precomputed_metadata, num_workers, _video_width, _video_height, _video_min_dimension, _audio_samples)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0m_video_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_video_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0m_video_min_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_video_min_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0m_audio_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_audio_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# we bookkeep the full version of video clips because we want to be able\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, video_paths, clip_length_in_frames, frames_between_clips, frame_rate, _precomputed_metadata, num_workers, _video_width, _video_height, _video_min_dimension, _video_max_dimension, _audio_samples, _audio_channels)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_audio_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_audio_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_precomputed_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_frame_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36m_compute_frame_pts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m         )\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                 \u001b[0;31m# store out-of-order samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# If the exception takes multiple arguments, don't try to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Caught ImportError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/video_utils.py\", line 67, in __getitem__\n    def __getitem__(self, idx: int) -> Tuple[List[int], Optional[float]]:\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/video.py\", line 389, in read_video_timestamps\n    if container.streams.video:\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/video.py\", line 42, in _check_av_available\n    raise av\nImportError: PyAV is not installed, and is necessary for the video operations in torchvision.\nSee https://github.com/mikeboers/PyAV#installation for instructions on how to\ninstall PyAV on your system.\n\n","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# create train loader (allowing batches and other extras)\n","\n","train_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n","                       step_between_clips=step_between_clips, train=True, transform=tfs)\n","#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","#                                           collate_fn=custom_collate)\n","# create test loader (allowing batches and other extras)\n","test_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n","                      step_between_clips=step_between_clips, train=False, transform=tfs)\n","#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n"," #                                         collate_fn=custom_collate)\n","\n"]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"0mnaQSM_EIaa","executionInfo":{"status":"aborted","timestamp":1652025504191,"user_tz":420,"elapsed":162,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get subset of data\n","#train_dataset = torch.utils.data.Subset(train_dataset, range(0, len(train_dataset), 8))\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                                         collate_fn=custom_collate)\n","print(train_loader)\n","#print(test_loader)"],"metadata":{"id":"e839lU7QDdoS","executionInfo":{"status":"aborted","timestamp":1652025504192,"user_tz":420,"elapsed":163,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":163,"status":"aborted","timestamp":1652025504193,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"},"user_tz":420},"id":"Pvfv3zh8xDyb"},"outputs":[],"source":["# Get a batch\n","input, _ = next(iter(train_loader))\n","\n","# Reverse process before displaying\n","input = input.cpu().numpy() * 255.0     \n","\n","for video in input[:4]:          # Loop over videos\n","    video = video.transpose(1, 2, 3, 0)\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n","        display(HBox([widgets.Image(value=gif.getvalue())]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVMYVahFxLLY","executionInfo":{"status":"aborted","timestamp":1652025504193,"user_tz":420,"elapsed":163,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["# The input video frames are grayscale, thus single channel\n","model = Seq2Seq(num_channels=3, num_kernels=64, \n","kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \n","frame_size=(64, 64), num_layers=3).to(device)\n","\n","optim = Adam(model.parameters(), lr=1e-6)\n","\n","# Binary Cross Entropy, target pixel values either 0 or 1\n","criterion = nn.BCELoss(reduction='sum')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXfGlVBuxNkZ","executionInfo":{"status":"aborted","timestamp":1652025504194,"user_tz":420,"elapsed":164,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["num_epochs = 3\n","\n","for epoch in range(1, num_epochs+1):\n","    \n","    train_loss = 0                                                 \n","    model.train()                                                  \n","    for batch_num, (input, target) in enumerate(train_loader, 1):  \n","        # input = input[:, 0, :, :, :].unsqueeze(1)\n","        output = model(input)      \n","        # print(output.detach().numpy())                         \n","        loss = criterion(output.flatten(), target.flatten())       \n","        loss.backward()                                            \n","        optim.step()                                               \n","        optim.zero_grad()                                           \n","        train_loss += loss.item()                                 \n","    train_loss /= len(train_loader.dataset)                       \n","\n","    val_loss = 0                                                 \n","    model.eval()                                                   \n","    with torch.no_grad():                                          \n","        for input, target in test_loader:                          \n","            output = model(input)                                   \n","            loss = criterion(output.flatten(), target.flatten())   \n","            val_loss += loss.item()                                \n","    val_loss /= len(test_loader.dataset)                            \n","\n","    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n","        epoch, train_loss, val_loss))\n","    \n","    num_epochs = 20\n","\n","for epoch in range(1, num_epochs+1):\n","    \n","    train_loss = 0                                                 \n","    model.train()                                                  \n","    for batch_num, (input, target) in enumerate(train_loader, 1):  \n","        output = model(input)                                     \n","        loss = criterion(output.flatten(), target.flatten())       \n","        loss.backward()                                            \n","        optim.step()                                               \n","        optim.zero_grad()                                           \n","        train_loss += loss.item()                                 \n","    train_loss /= len(train_loader.dataset)                       \n","\n","    val_loss = 0                                                 \n","    model.eval()                                                   \n","    with torch.no_grad():                                          \n","        for input, target in test_loader:                          \n","            output = model(input)                                   \n","            loss = criterion(output.flatten(), target.flatten())   \n","            val_loss += loss.item()                                \n","    val_loss /= len(test_loader.dataset)                            \n","\n","    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n","        epoch, train_loss, val_loss))"]},{"cell_type":"code","source":["with io.BytesIO() as gif:\n","    new_input = np.array(input[0, :, :, :, :]) * 255.0\n","    new_input = new_input.transpose(1, 2, 3, 0)\n","    print(input.shape)\n","    imageio.mimsave(gif, new_input, \"GIF\", fps = 5)    \n","    input_gif = gif.getvalue()\n","# Write target video as gif\n","with io.BytesIO() as gif:\n","    new_target = np.array(target[0, :, :, :].unsqueeze(0))* 255.0\n","    new_target = new_target.transpose(0, 2, 3, 1)\n","    print(target.shape)\n","    imageio.mimsave(gif, new_target, \"GIF\", fps = 5)    \n","    target_gif = gif.getvalue()\n","\n","# Write output video as gif\n","with io.BytesIO() as gif:\n","    new_output = output[0, :, :, :].unsqueeze(0).detach().numpy()* 255.0\n","    new_output = new_output.transpose(0, 2, 3, 1)\n","    imageio.mimsave(gif, new_output, \"GIF\", fps = 5)    \n","    output_gif = gif.getvalue()\n","\n","display(HBox([widgets.Image(value=input_gif), widgets.Image(value=target_gif), \n","                  widgets.Image(value=output_gif)]))\n"],"metadata":{"id":"I-guJIqQ_Cmg","executionInfo":{"status":"aborted","timestamp":1652025504194,"user_tz":420,"elapsed":164,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN5L8ynwxWQO","executionInfo":{"status":"aborted","timestamp":1652025504195,"user_tz":420,"elapsed":165,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["def collate_test(batch):\n","\n","    # Last 1 frames are target\n","    target = np.array(batch)[:,1:]                     \n","    \n","    # Add channel dim, scale pixels between 0 and 1, send to GPU\n","    batch = torch.tensor(batch)#.unsqueeze(1)          \n","    batch = batch / 255.0                             \n","    batch = batch.to(device)                          \n","    return batch, target\n","\n","# Test Data Loader\n","# test_loader = DataLoader(test_data,shuffle=True, \n","#                          batch_size=3, collate_fn=collate_test)\n","\n","# Get a batch\n","batch, target = next(iter(test_loader))\n","\n","# Initialize output sequence\n","output = np.zeros(target.shape, dtype=np.uint8)\n","\n","# Loop over timesteps\n","for timestep in range(target.shape[1]):\n","  input = batch[:,:,timestep:timestep+10]   \n","  output[:,timestep]=(model(input).squeeze(1).cpu()>0.5)*255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pNFdQSWxYi0","executionInfo":{"status":"aborted","timestamp":1652025504195,"user_tz":420,"elapsed":165,"user":{"displayName":"Dana Murphy","userId":"12099415617376626403"}}},"outputs":[],"source":["for tgt, out in zip(target, output):       # Loop over samples\n","    tgt = tgt.transpose(1, 2, 3, 0)\n","    out = out.transpose(1, 2, 3, 0)\n","    # Write target video as gif\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif, tgt, \"GIF\", fps = 5)    \n","        target_gif = gif.getvalue()\n","\n","    # Write output video as gif\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif, out, \"GIF\", fps = 5)    \n","        output_gif = gif.getvalue()\n","\n","    display(HBox([widgets.Image(value=target_gif), \n","                  widgets.Image(value=output_gif)]))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"UCF_Training_Subset.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"80184dae3d69414d9fcd3adf809a7e67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f98046bf0baf4fe3a1456177d191e098","IPY_MODEL_1040e8183e04482b8d400cffca68e609","IPY_MODEL_2d775ac035254e42ac8b3373372d31db"],"layout":"IPY_MODEL_2d3d7a862cff4e5bb2a7e39040d946a9"}},"f98046bf0baf4fe3a1456177d191e098":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df11f03a91854b8f811d3875327b8714","placeholder":"​","style":"IPY_MODEL_e04bd0d3cd45401fb8727373f1aa004f","value":"  0%"}},"1040e8183e04482b8d400cffca68e609":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_23324c2d6aa64b00821fdcc58d75ce53","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bd355d158ca42989a55e17f671862d0","value":0}},"2d775ac035254e42ac8b3373372d31db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79470611b15041fcb197cea6b8cb94f6","placeholder":"​","style":"IPY_MODEL_f21cb511a5204971966049436380920d","value":" 0/32 [00:00&lt;?, ?it/s]"}},"2d3d7a862cff4e5bb2a7e39040d946a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df11f03a91854b8f811d3875327b8714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04bd0d3cd45401fb8727373f1aa004f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23324c2d6aa64b00821fdcc58d75ce53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bd355d158ca42989a55e17f671862d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79470611b15041fcb197cea6b8cb94f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f21cb511a5204971966049436380920d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}