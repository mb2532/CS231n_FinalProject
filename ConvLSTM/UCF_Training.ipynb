{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UCF_Training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d598a0ef587b4e198fb3bf2efdfed6e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca837f407f0f4342ab461925c735e871","IPY_MODEL_f218b5aab6564cc6bba6cee347df13a8","IPY_MODEL_4508db4c1bec4b11a1d24acb668a5f37"],"layout":"IPY_MODEL_30b026009a9e46449110f547f3643b0a"}},"ca837f407f0f4342ab461925c735e871":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9810440ae4a48a9b78e97b48015df0f","placeholder":"​","style":"IPY_MODEL_6608ff288ee045aa9ec6db9c0353ff52","value":"100%"}},"f218b5aab6564cc6bba6cee347df13a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bc5d69761864d86a8d7276ec8753753","max":833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f542a35cadaf46c1b19689776b6565ee","value":833}},"4508db4c1bec4b11a1d24acb668a5f37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3442865db6be49c58e6bd8c7a7f32157","placeholder":"​","style":"IPY_MODEL_2cf638ed91e7478db23dc5bb63bfc3f0","value":" 833/833 [1:55:33&lt;00:00,  7.35s/it]"}},"30b026009a9e46449110f547f3643b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9810440ae4a48a9b78e97b48015df0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6608ff288ee045aa9ec6db9c0353ff52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bc5d69761864d86a8d7276ec8753753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f542a35cadaf46c1b19689776b6565ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3442865db6be49c58e6bd8c7a7f32157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cf638ed91e7478db23dc5bb63bfc3f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwCYzowylIjb","executionInfo":{"status":"ok","timestamp":1651796805488,"user_tz":420,"elapsed":863,"user":{"displayName":"Megan Backus","userId":"05983633256724241986"}},"outputId":"3d5811ca-c96f-4c19-8a4d-19a9d53807a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch,torchvision\n","print(f\"torch version：{torch.__version__}\")\n","print(f\"torchvision version: {torchvision.__version__}\")\n","print(f\"CUDA Version: {torch.version.cuda}\")\n","print(f\"cuDNN version is: {torch.backends.cudnn.version()}\")"],"metadata":{"id":"-EQThYJflKmi","executionInfo":{"status":"ok","timestamp":1651796806408,"user_tz":420,"elapsed":921,"user":{"displayName":"Megan Backus","userId":"05983633256724241986"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"afd608cf-4a62-48a4-9f31-7266a07e858e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch version：1.11.0+cu113\n","torchvision version: 0.12.0+cu113\n","CUDA Version: 11.3\n","cuDNN version is: 8200\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torchvision.transforms as T"],"metadata":{"id":"67FHvRnRj8pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pytorchvideo av\n","!pip install av\n","import pytorchvideo\n","import av\n","print(av.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCTqB2mYjnGf","executionInfo":{"status":"ok","timestamp":1651796813891,"user_tz":420,"elapsed":7486,"user":{"displayName":"Megan Backus","userId":"05983633256724241986"}},"outputId":"cf9ab1e9-42c6-442e-c448-2da14c6e65ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorchvideo in /usr/local/lib/python3.7/dist-packages (0.1.5)\n","Requirement already satisfied: av in /usr/local/lib/python3.7/dist-packages (9.2.0)\n","Requirement already satisfied: parameterized in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (0.8.1)\n","Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (0.1.9)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (2.6.3)\n","Requirement already satisfied: fvcore in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (0.1.5.post20220504)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.21.6)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (6.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->pytorchvideo) (2.4.0)\n","Requirement already satisfied: av in /usr/local/lib/python3.7/dist-packages (9.2.0)\n","9.2.0\n"]}]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"_zigZRXVkBVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FOLDERNAME = 'CS_231n_Project/ConvLSTM'\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHBH7qy9uzQS","executionInfo":{"status":"ok","timestamp":1651796814116,"user_tz":420,"elapsed":13,"user":{"displayName":"Megan Backus","userId":"05983633256724241986"}},"outputId":"3adbcf58-71ff-46e2-87cf-39976fe7b3ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CS_231n_Project/ConvLSTM\n"]}]},{"cell_type":"code","source":["from torchvision.datasets import UCF101\n","\n","# These are some minimal variables used to configure and load the dataset:\n","ucf_data_dir = \"../UCF101/UCF-101\"\n","ucf_label_dir = \"../UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist\"\n","frames_per_clip = 5\n","step_between_clips = 1\n","batch_size = 32"],"metadata":{"id":"4bu5-fKNlMg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from Seq2Seq import Seq2Seq\n","from torch.utils.data import DataLoader\n","\n","import io\n","import imageio\n","from ipywidgets import widgets, HBox\n","\n","# Use GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"ZzkC66YOt4tf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfs = T.Compose([\n","    # TODO: this should be done by a video-level transfrom when PyTorch provides transforms.ToTensor() for video\n","    # scale in [0, 1] of type float\n","    T.Lambda(lambda x: x / 255.),\n","    # reshape into (T, C, H, W) for easier convolutions\n","    T.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n","    # rescale to the most common size\n","    T.Lambda(lambda x: nn.functional.interpolate(x, (240, 320))),\n","])"],"metadata":{"id":"56A6U7vIO_lF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def custom_collate(batch):\n","#     filtered_batch = []\n","#     for video, _, label in batch:\n","#         filtered_batch.append((video, label))\n","#     return torch.utils.data.dataloader.default_collate(filtered_batch)"],"metadata":{"id":"rn31TitrkRGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_collate(batch):\n","\n","    # Add channel dim, scale pixels between 0 and 1, send to GPU\n","    filtered_batch = []\n","    for video, _, _ in batch:\n","      filtered_batch.append(video)\n","\n","    batch = torch.tensor(filtered_batch).unsqueeze(1)     \n","    batch = batch / 255.0                        \n","    batch = batch.to(device)                     \n","\n","    # Randomly pick 4 frames as input, 5th frame is target\n","    rand = np.random.randint(5,10)                     \n","    return batch[:,:,rand-5:rand], batch[:,:,rand]     \n"],"metadata":{"id":"TAGfg2Tfv6RA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create train loader (allowing batches and other extras)\n","train_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n","                       step_between_clips=step_between_clips, train=True, transform=tfs)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                                           collate_fn=custom_collate)\n","# create test loader (allowing batches and other extras)\n","test_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n","                      step_between_clips=step_between_clips, train=False, transform=tfs)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n","                                          collate_fn=custom_collate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407,"referenced_widgets":["d598a0ef587b4e198fb3bf2efdfed6e4","ca837f407f0f4342ab461925c735e871","f218b5aab6564cc6bba6cee347df13a8","4508db4c1bec4b11a1d24acb668a5f37","30b026009a9e46449110f547f3643b0a","d9810440ae4a48a9b78e97b48015df0f","6608ff288ee045aa9ec6db9c0353ff52","3bc5d69761864d86a8d7276ec8753753","f542a35cadaf46c1b19689776b6565ee","3442865db6be49c58e6bd8c7a7f32157","2cf638ed91e7478db23dc5bb63bfc3f0"]},"id":"oY8nKu8NPETm","outputId":"75ee6b3e-bfe5-4712-f443-5287fda0ed36","executionInfo":{"status":"error","timestamp":1651803748968,"user_tz":420,"elapsed":6934857,"user":{"displayName":"Megan Backus","userId":"05983633256724241986"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d598a0ef587b4e198fb3bf2efdfed6e4"}},"metadata":{}},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f670a0894c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create train loader (allowing batches and other extras)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataset = UCF101(ucf_data_dir, ucf_label_dir, frames_per_clip=frames_per_clip,\n\u001b[0;32m----> 3\u001b[0;31m                        step_between_clips=step_between_clips, train=True, transform=tfs)\n\u001b[0m\u001b[1;32m      4\u001b[0m train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n\u001b[1;32m      5\u001b[0m                                            collate_fn=custom_collate)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/ucf101.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annotation_path, frames_per_clip, step_between_clips, frame_rate, fold, train, transform, _precomputed_metadata, num_workers, _video_width, _video_height, _video_min_dimension, _audio_samples)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# video clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_video_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_clips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/ucf101.py\u001b[0m in \u001b[0;36m_select_fold\u001b[0;34m(self, video_list, annotation_path, fold, train)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mselected_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/trainlist01.txt'"]}]},{"cell_type":"code","source":["# Get a batch\n","input, _ = next(iter(test_loader))\n","\n","# Reverse process before displaying\n","input = input.cpu().numpy() * 255.0     \n","\n","for video in input.squeeze(1)[:3]:          # Loop over videos\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n","        display(HBox([widgets.Image(value=gif.getvalue())]))"],"metadata":{"id":"Pvfv3zh8xDyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = Seq2Seq(num_channels=3, num_kernels=64, \n","kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \n","frame_size=(64, 64), num_layers=3).to(device)\n","\n","optim = Adam(model.parameters(), lr=1e-4)\n","\n","# Binary Cross Entropy, target pixel values either 0 or 1\n","criterion = nn.BCELoss(reduction='sum')"],"metadata":{"id":"lVMYVahFxLLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 20\n","\n","for epoch in range(1, num_epochs+1):\n","    \n","    train_loss = 0                                                 \n","    model.train()                                                  \n","    for batch_num, (input, target) in enumerate(train_loader, 1):  \n","        output = model(input)                                     \n","        loss = criterion(output.flatten(), target.flatten())       \n","        loss.backward()                                            \n","        optim.step()                                               \n","        optim.zero_grad()                                           \n","        train_loss += loss.item()                                 \n","    train_loss /= len(train_loader.dataset)                       \n","\n","    val_loss = 0                                                 \n","    model.eval()                                                   \n","    with torch.no_grad():                                          \n","        for input, target in test_loader:                          \n","            output = model(input)                                   \n","            loss = criterion(output.flatten(), target.flatten())   \n","            val_loss += loss.item()                                \n","    val_loss /= len(test_loader.dataset)                            \n","\n","    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n","        epoch, train_loss, val_loss))\n","    \n","    num_epochs = 20\n","\n","for epoch in range(1, num_epochs+1):\n","    \n","    train_loss = 0                                                 \n","    model.train()                                                  \n","    for batch_num, (input, target) in enumerate(train_loader, 1):  \n","        output = model(input)                                     \n","        loss = criterion(output.flatten(), target.flatten())       \n","        loss.backward()                                            \n","        optim.step()                                               \n","        optim.zero_grad()                                           \n","        train_loss += loss.item()                                 \n","    train_loss /= len(train_loader.dataset)                       \n","\n","    val_loss = 0                                                 \n","    model.eval()                                                   \n","    with torch.no_grad():                                          \n","        for input, target in test_loader:                          \n","            output = model(input)                                   \n","            loss = criterion(output.flatten(), target.flatten())   \n","            val_loss += loss.item()                                \n","    val_loss /= len(test_loader.dataset)                            \n","\n","    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n","        epoch, train_loss, val_loss))"],"metadata":{"id":"wXfGlVBuxNkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_test(batch):\n","\n","    # Last 5 frames are target\n","    target = np.array(batch)[:,5:]                     \n","    \n","    # Add channel dim, scale pixels between 0 and 1, send to GPU\n","    batch = torch.tensor(batch).unsqueeze(1)          \n","    batch = batch / 255.0                             \n","    batch = batch.to(device)                          \n","    return batch, target\n","\n","# Test Data Loader\n","# test_loader = DataLoader(test_data,shuffle=True, \n","#                          batch_size=3, collate_fn=collate_test)\n","\n","# Get a batch\n","batch, target = next(iter(test_loader))\n","\n","# Initialize output sequence\n","output = np.zeros(target.shape, dtype=np.uint8)\n","\n","# Loop over timesteps\n","for timestep in range(target.shape[1]):\n","  input = batch[:,:,timestep:timestep+10]   \n","  output[:,timestep]=(model(input).squeeze(1).cpu()>0.5)*255.0"],"metadata":{"id":"VN5L8ynwxWQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for tgt, out in zip(target, output):       # Loop over samples\n","    \n","    # Write target video as gif\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif, tgt, \"GIF\", fps = 5)    \n","        target_gif = gif.getvalue()\n","\n","    # Write output video as gif\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif, out, \"GIF\", fps = 5)    \n","        output_gif = gif.getvalue()\n","\n","    display(HBox([widgets.Image(value=target_gif), \n","                  widgets.Image(value=output_gif)]))"],"metadata":{"id":"8pNFdQSWxYi0"},"execution_count":null,"outputs":[]}]}